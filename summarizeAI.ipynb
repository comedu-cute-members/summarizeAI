{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:00:17.590285Z",
     "start_time": "2022-11-16T05:00:09.021259Z"
    },
    "id": "DCoRPSSDMOSN"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "임포트\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gov2I1jKMOSO"
   },
   "source": [
    "\"\"\"\n",
    "데이터 파싱\n",
    "\"\"\"\n",
    "values = []\n",
    "def parse_file_content(root_path, is_summary, section, file_count):\n",
    "    folder_sort = [\"News Articles\", \"Summaries\"]\n",
    "\n",
    "    if is_summary:\n",
    "        index = 1\n",
    "    else:\n",
    "        index = 0\n",
    "   \n",
    "    # 파일 열기\n",
    "    file_name = ('%d' % file_count).zfill(3) + '.txt'\n",
    "    path = \"%s/%s/%s/%s\" % (root_path, folder_sort[index], section, file_name)\n",
    "    file = open(path, 'r')\n",
    "\n",
    "    # 파일 읽어서 content에 넣기\n",
    "    lines = file.readlines()\n",
    "    content = ''\n",
    "    for line in lines:\n",
    "        content = content + ' ' + line.strip()\n",
    "\n",
    "    # 파일 닫기\n",
    "    file.close()\n",
    "\n",
    "    # 파일 내용 반환\n",
    "    return content\n",
    "  \n",
    "def parse_folder(root_path, section):\n",
    "    folder_path = \"%s/Summaries/%s/\" % (root_path, section)\n",
    "    file_list = os.listdir(folder_path)\n",
    "    max_file_count = len(file_list)\n",
    "  \n",
    "    for i in range(1, max_file_count + 1):\n",
    "        # Text 데이터 가져오기\n",
    "        text = parse_file_content(root_path=root_path, is_summary=False, section=section, file_count=i)\n",
    "    \n",
    "        # Summary 데이터 가져오기\n",
    "        summary = parse_file_content(root_path=root_path, is_summary=True, section=section, file_count=i)\n",
    "\n",
    "        # values에 데이터 넣기\n",
    "        values.append([text, summary])\n",
    "\n",
    "# 모든 섹션들 데이터에 넣기\n",
    "root_path = \"./BBC News Summary\"\n",
    "parse_folder(root_path, \"business\")\n",
    "parse_folder(root_path, \"entertainment\")\n",
    "parse_folder(root_path, \"politics\")\n",
    "parse_folder(root_path, \"sport\")\n",
    "parse_folder(root_path, \"tech\")\n",
    "\n",
    "# 데이터프레임 만들어 csv 파일로 저장\n",
    "df = pd.DataFrame(values)\n",
    "df.columns = [\"Text\", \"Summary\"]\n",
    "df.to_csv(\"News.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:00:20.773471Z",
     "start_time": "2022-11-16T05:00:20.673218Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DomdcrVhMOSP",
    "outputId": "e99191ce-04a1-4739-da31-b88d24a54dbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 갯수 : 2225\n",
      " \"We need at least $20bn (짙10.6bn) in investment and part of this has to come as foreign direct investment,\" said Mr Maran.Potential foreign investors will however need government approval before they increase their stake beyond 49%, Mr Maran said.Communications Minister Dayanidhi Maran said that there is a need to fund the fast-growing mobile market.India has raised the limit for foreign direct investment in telecoms companies from 49% to 74%.Investment bank Morgan Stanley has forecast that India's mobile market is likely to grow by about 40% a year until 2007.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "전처리\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"News.csv\")\n",
    "\n",
    "# print(df)\n",
    "print(\"전체 데이터 갯수 :\", len(df))\n",
    "# print(df.isnull().sum())\n",
    "print(df[\"Summary\"][18]) # UTF-8 인코딩이 유로 표시를 '짙'으로 바꿈, 영어 빼고 제외시킬 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:01:21.764676Z",
     "start_time": "2022-11-16T05:00:25.586671Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjVw7TNeMOSQ",
    "outputId": "2499f0e3-7478-4d65-bd37-83f5fa5f685e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ad sales boost Time Warner profit  Quarterly profits at US media giant TimeWarner jumped % to $ . bn m for the three months to December ,  from $m yearearlier .   The firm ,  which is now one of the biggest investors in Google ,  benefited from sales of highspeed internet connections and higher advert sales .  TimeWarner said fourth quarter sales rose % to $ . bn from $ . bn .  Its profits were buoyed by oneoff gains which offset a profit dip at Warner Bros ,  and less users for AOL .   Time Warner said on Friday that it now owns % of searchengine Google .  But its own internet business ,  AOL ,  had has mixed fortunes .  It lost  ,  subscribers in the fourth quarter profits were lower than in the preceding three quarters .  However ,  the company said AOLs underlying profit before exceptional items rose % on the back of stronger internet advertising revenues .  It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOLs existing customers for highspeed broadband .  TimeWarner also has to restate  and  results following a probe by the US Securities Exchange Commission SEC ,  which is close to concluding .   Time Warners fourth quarter profits were slightly better than analysts expectations .  But its film division saw profits slump % to $m ,  helped by boxoffice flops Alexander and Catwoman ,  a sharp contrast to yearearlier ,  when the third and final film in the Lord of the Rings trilogy boosted results .  For the fullyear ,  TimeWarner posted a profit of $ . bn ,  up % from its  performance ,  while revenues grew  . % to $ . bn .  Our financial performance was strong ,  meeting or exceeding all of our fullyear objectives and greatly enhancing our flexibility ,  chairman and chief executive Richard Parsons said .  For  ,  TimeWarner is projecting operating earnings growth of around % ,  and also expects higher revenue and wider profit margins .   TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators .  It has already offered to pay $m to settle charges ,  in a deal that is under review by the SEC .  The company said it was unable to estimate the amount it needed to set aside for legal reserves ,  which it previously set at $m .  It intends to adjust the way it accounts for a deal with German music publisher Bertelsmanns purchase of a stake in AOL Europe ,  which it had reported as advertising revenue .  It will now book the sale of its stake in AOL Europe as a loss on the value of that stake .\n",
      "TimeWarner said fourth quarter sales rose 2% to $11 . 1bn from $10 . 9bn . For the full year ,  TimeWarner posted a profit of $3 . 36bn ,  up 27% from its 2003 performance ,  while revenues grew 6 . 4% to $42 . 09bn . Quarterly profits at US media giant TimeWarner jumped 76% to $1 . 13bn  600m  for the three months to December ,  from $639m year earlier . However ,  the company said AOL s underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues . Its profits were buoyed by one off gains which offset a profit dip at Warner Bros ,  and less users for AOL . For 2005 ,  TimeWarner is projecting operating earnings growth of around 5% ,  and also expects higher revenue and wider profit margins . It lost 464 , 000 subscribers in the fourth quarter profits were lower than in the preceding three quarters . Time Warner s fourth quarter profits were slightly better than analysts  expectations .\n",
      "시작 토큰 번호 : [8127]\n",
      "종료 토큰 번호 : [8128]\n",
      "단어 집합의 크기 : 8129\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for content in df['Text']:\n",
    "    content = re.sub(r\"([?.!,])\", r\" \\1 \", content)\n",
    "    content = re.sub(\"[^ A-Za-z?.!,$%]+\", '', content)\n",
    "    content = content.strip()\n",
    "    texts.append(content)\n",
    "\n",
    "summaries = []\n",
    "for content in df['Summary']:\n",
    "    content = re.sub(r\"([?.!,])\", r\" \\1 \", content)\n",
    "    content = re.sub(\"[^ A-Za-z0-9?.!,$%]+\", ' ', content)\n",
    "    content = content.strip()\n",
    "    summaries.append(content)\n",
    "\n",
    "# 단어 모음 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    texts + summaries, target_vocab_size=2**13\n",
    ")\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]  # <sos> 와 <eos>\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "\n",
    "print(\"시작 토큰 번호 :\", START_TOKEN)\n",
    "print(\"종료 토큰 번호 :\", END_TOKEN)\n",
    "print(\"단어 집합의 크기 :\", VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:58:01.892699Z",
     "start_time": "2022-11-16T05:58:01.846405Z"
    },
    "id": "t1HBzUgiMOSR"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SubwordTextEncoder' object has no attribute 'text_to_sequences'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36168\\308357538.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenized_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenized_outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mtok_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtok_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_and_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36168\\308357538.py\u001b[0m in \u001b[0;36mtokenize_and_filter\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtokenized_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenized_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtokenized_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtokenized_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SubwordTextEncoder' object has no attribute 'text_to_sequences'"
     ]
    }
   ],
   "source": [
    "MAX_INPUT_LENGTH = 1000\n",
    "MAX_OUTPUT_LENGTH = 600\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    \n",
    "    tokenized_inputs = tokenizer.text_to_sequences(inputs)\n",
    "    tokenized_outputs = tokenizer.text_to_sequences(outputs)\n",
    "\n",
    "    # 시작 토큰(sos)과 종료 토큰(eos) 포함\n",
    "    for (content1, content2) in zip(tokenized_inputs, tokenized_outputs):\n",
    "        content1 = START_TOKEN + content1 + END_TOKEN\n",
    "        content2 = START_TOKEN + content2 + END_TOKEN\n",
    "\n",
    "        tokenized_inputs.append(content1)\n",
    "        tokenized_outputs.append(content2)\n",
    "    \n",
    "    # 길이를 1000, 600으로 맞춘다. 더 짧은 배열은 뒤에 0을 추가한다.\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_inputs, maxlen=MAX_INPUT_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(tokenized_outputs, maxlen=MAX_OUTPUT_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "tok_texts, tok_summaries = tokenize_and_filter(texts, summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:01:25.511095Z",
     "start_time": "2022-11-16T05:01:25.498335Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlGEoN1cMOSR",
    "outputId": "aec62ad9-41fa-4e6d-9ee3-5cd18da70315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기사 데이터의 크기(shape) : (2225, 1000)\n",
      "요약 데이터의 크기(shape) : (2225, 600)\n",
      "TimeWarner said fourth quarter sales rose 2% to $11 . 1bn from $10 . 9bn . For the full year ,  TimeWarner posted a profit of $3 . 36bn ,  up 27% from its 2003 performance ,  while revenues grew 6 . 4% to $42 . 09bn . Quarterly profits at US media giant TimeWarner jumped 76% to $1 . 13bn  600m  for the three months to December ,  from $639m year earlier . However ,  the company said AOL s underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues . Its profits were buoyed by one off gains which offset a profit dip at Warner Bros ,  and less users for AOL . For 2005 ,  TimeWarner is projecting operating earnings growth of around 5% ,  and also expects higher revenue and wider profit margins . It lost 464 , 000 subscribers in the fourth quarter profits were lower than in the preceding three quarters . Time Warner s fourth quarter profits were slightly better than analysts  expectations .\n",
      "[8127 5550 7972 5181   82   18  750  680  248 1089 7921  166  118  145\n",
      " 3210   11 7634 7903  461  145 1191   11 7928  352   11  633    1  624\n",
      "  116    2 5550 7972 5181   82 6851    6 1709  134  145 7922   11 7922\n",
      " 7868    2   58 4561  166   37   47 4284 2516    2  152 4483 3504 7925\n",
      "   11 7923  166  118  145 7923 7921   11 7919 7928  352   11 7952 1183\n",
      " 4220 4144  961   30   74  519  824 5550 7972 5181   82 7506 7903 7926\n",
      " 7925  166  118  145 7920   11 4058  352   10 2409 4876   10   12    1\n",
      "  141  324    3  792    2  461  145 7925 7922 7928   76   95 5167   11\n",
      "  340    2    1  209   18 4672 1136    9  527 6878 1709  149 7532 5377\n",
      " 6039 7903 1089 7927  166   16    1  136    5 5723  639 5414 5129 3316\n",
      "   11  247  961   52  788 1895   27   31   70  179 6374 7903   45 3270\n",
      "  686    6 1709 3452 7903   30 5181   82 3173 7986    2    7  273  317\n",
      "   12 4672 7947   11  633 2098    2 5550 7972 5181   82   13 2004   24\n",
      " 1573 2811  275    5  329 7924 5334    7   53 3238  880 4005    7 4987\n",
      " 1709 4234 7986   11   65  425 7923 7925 7923  276  626 6646 7903    8\n",
      "    1  750  680  961   52 1427   81    8    1 3080 3812  102  141 1173\n",
      " 7986   11 5550   39 5181   82    9  750  680  961   52 3672  362   81\n",
      " 7358 7986   10 7323 7986   41 8128    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(\"기사 데이터의 크기(shape) :\", tok_texts.shape)\n",
    "print(\"요약 데이터의 크기(shape) :\", tok_summaries.shape)\n",
    "\n",
    "print(summaries[0])\n",
    "print(tok_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:01:25.558280Z",
     "start_time": "2022-11-16T05:01:25.513017Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LTxfXCesMOSR",
    "outputId": "e8ea550f-08bf-46b3-9ff6-d23c2a38f59c"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'enc_inputs': tok_texts,\n",
    "        'dec_inputs': tok_summaries[:, :-1] # 마지막 패딩 0 제거\n",
    "    },\n",
    "    {\n",
    "        'outputs': tok_summaries[:, 1:] # 시작 토큰 제거\n",
    "    }\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "test_dataset = dataset.take(49)   # 앞에서 49개를 테스트 데이터로 뺌\n",
    "dataset = dataset.skip(49)   # 나머지 2176개가 학습 데이터가 됨\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)  # 메모리 사용을 위한 prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3K1gJeoMOSS"
   },
   "source": [
    "## transformer 아키텍처 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:03:07.890488Z",
     "start_time": "2022-11-16T05:03:07.877429Z"
    },
    "id": "YgbOK1gCMOSS"
   },
   "outputs": [],
   "source": [
    "# 인코더&디코더(포지셔널 인코딩)\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model, name=\"Positional_Encoding\"):\n",
    "        super(PositionalEncoding, self).__init__(name=name)\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        return position * (1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32)))\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position = tf.range(position, dtype = tf.float32)[:, tf.newaxis],\n",
    "            i = tf.range(d_model, dtype = tf.float32)[tf.newaxis, :],\n",
    "            d_model = d_model\n",
    "        )\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        angle_rads = np.zeros(angle_rads.shape)\n",
    "        angle_rads[:, 0::2] = sines # even index -> sin\n",
    "        angle_rads[:, 1::2] = cosines # odd index  -> cos\n",
    "        pos_encoding = tf.constant(angle_rads)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:35:53.024043Z",
     "start_time": "2022-11-16T05:35:53.003290Z"
    },
    "id": "3fNaMUFBMOST"
   },
   "outputs": [],
   "source": [
    "#인코더(스케일드 닷 프로덕트 어텐션)\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \n",
    "    # split head 상태로 진행\n",
    "    \n",
    "    multiple_QandK = tf.matmul(query, key, transpose_b=True) # 행렬 곱셈\n",
    "    d_k = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    \n",
    "    # 어텐션 에너지\n",
    "    energy = multiple_QandK / (d_k ** 0.5)\n",
    "\n",
    "    if mask is not None :\n",
    "        energy += (mask * -1e10)\n",
    "        \n",
    "    # 어텐션 스코어\n",
    "    attention_weights = tf.nn.softmax(energy, axis = -1)\n",
    "    \n",
    "    # scaled dot product attention\n",
    "    attention_outputs = tf.matmul(attention_weights, value) # 행렬 곱셈\n",
    "    return attention_outputs, attention_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:01:29.264128Z",
     "start_time": "2022-11-16T05:01:29.255546Z"
    },
    "id": "naDI940YMOST"
   },
   "outputs": [],
   "source": [
    "# 패딩 마스크 생성\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x,0), tf.float32) # [[1,2,0,2,1]] => [[0.,0.,1.,0.,0.]]\n",
    "    return mask[:,tf.newaxis, tf.newaxis, :] # 차원 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:01:29.902103Z",
     "start_time": "2022-11-16T05:01:29.890095Z"
    },
    "id": "lduANA-XMOSU"
   },
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1] # [[1,2,0]] => 3\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0) # 모든 원소가 1인 하삼각행렬\n",
    "    padding_mask = create_padding_mask(x) # x에서 0이었던 부분만 1로 바꿔진 행렬\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:01:30.604936Z",
     "start_time": "2022-11-16T05:01:30.590547Z"
    },
    "id": "0J4ViLosMOSU"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"Multi_Head_Attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0 # d_model 사이즈의 행렬을 num_heads로 나눠야하기 때문\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # WQ, WK, WV 정의 : d_model 길이의 밀집층(가중치 행렬)\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model) # WQ (size: d_model * d_k)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model) # WK (size: d_model * d_k)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model) # WV (size: d_model * d_v)\n",
    "        \n",
    "        # WO\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model) # size: transpose hd_v * d_model\n",
    "    \n",
    "    # num_heads 개수로 행렬 나누기\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0,2,1,3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # 신경망 지나기\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 헤드 나누기\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        #스케일 닷 프로덕트\n",
    "        scaled_dot_attention, attention_weight = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_dot_attention = tf.transpose(scaled_dot_attention, perm=[0,2,1,3])\n",
    "\n",
    "        # concat \n",
    "        concat_attention = tf.reshape(scaled_dot_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        # WO 밀집층 레이어 지나기\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:01:34.876390Z",
     "start_time": "2022-11-16T05:01:34.854809Z"
    },
    "id": "aD6bxOfHMOSU"
   },
   "outputs": [],
   "source": [
    "# 인코더 레이어 (tensorflow model)\n",
    "def encoder_layer(dff, d_model, num_heads, dropout, name = \"encoder_layer\"):\n",
    "    \n",
    "    inputs = tf.keras.Input(shape = (None, d_model), name = \"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape = (1, 1, None), name = \"padding_mask\")\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name = \"attention\")({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention_norm)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention_norm + outputs)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:30:22.507550Z",
     "start_time": "2022-11-16T05:30:22.484248Z"
    },
    "id": "xLctdIq6MOSU"
   },
   "outputs": [],
   "source": [
    "# real 인코더\n",
    "def encoder(\n",
    "    vocab_size, \n",
    "    num_layers, \n",
    "    dff, d_model, \n",
    "    num_heads, \n",
    "    dropout, \n",
    "    name=\"encoder\"):\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    input_embedded = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model)(inputs)\n",
    "    input_embedded *= tf.cast(d_model, tf.float32) ** 0.5\n",
    "    input_positional_encoded = PositionalEncoding(position=vocab_size, d_model=d_model)(input_embedded)\n",
    "    enc_outputs = tf.keras.layers.Dropout(rate=dropout)(input_positional_encoded)\n",
    "\n",
    "    # encoder layer * N\n",
    "    for i in range(num_layers):\n",
    "        enc_outputs = encoder_layer(\n",
    "            dff=dff, \n",
    "            d_model=d_model, \n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_\"+str(i),)([enc_outputs, padding_mask])\n",
    "            \n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=enc_outputs, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaP9rLUKMOSV"
   },
   "source": [
    "## 디코더 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:19:04.352642Z",
     "start_time": "2022-11-16T05:19:04.329825Z"
    },
    "id": "gbsnjclOMOSV"
   },
   "outputs": [],
   "source": [
    "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    \n",
    "    # mask\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # masked multi-head attention\n",
    "    attention1 = MultiHeadAttention(d_model, num_heads, name=\"attention_1\")(inputs={\"query\":inputs, \"key\":inputs, \"value\":inputs, \"mask\":look_ahead_mask})\n",
    "\n",
    "    # add, normorlization\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # multi-head attention (encoder-decoder attention)\n",
    "    attention2 = MultiHeadAttention(d_model, num_heads, name=\"attention_2\")(inputs={\"query\":attention1, \"key\":enc_outputs, \"value\":enc_outputs, \"mask\":padding_mask})\n",
    "\n",
    "    # dropout, add, normorlization\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + attention1)\n",
    "    \n",
    "    # feed forward (dense layer)\n",
    "    outputs = tf.keras.layers.Dense(units=dff, activation=\"relu\")(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)  # activation : None\n",
    "\n",
    "    # dropout, add, normorlization\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention2 + outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:19:04.758391Z",
     "start_time": "2022-11-16T05:19:04.748284Z"
    },
    "id": "jgqFA45fMOSV"
   },
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=\"decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # output embedding\n",
    "    output_embedded = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=d_model)(inputs)\n",
    "    output_embedded *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    output_embedded = PositionalEncoding(position=vocab_size, d_model=d_model)(output_embedded)\n",
    "    output_embedded = tf.keras.layers.Dropout(rate=dropout)(output_embedded)\n",
    "\n",
    "    # decoder layer * N\n",
    "    for i in range(num_layers):\n",
    "        dec_outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads, dropout=dropout, name=\"decoder_layer_\"+str(i),)(inputs=[output_embedded, enc_outputs, look_ahead_mask, padding_mask])\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask], outputs=dec_outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sTDhtTNMOSV"
   },
   "source": [
    "## 트랜스포머"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:26:56.151537Z",
     "start_time": "2022-11-16T05:26:56.135983Z"
    },
    "id": "OCUDKICKMOSV"
   },
   "outputs": [],
   "source": [
    "def transformer(vocab_size, num_layers, dff, d_model, num_heads, dropout, name=\"transformer\"):\n",
    "    \n",
    "    # encoder input (type: keras tensor)\n",
    "    enc_inputs = tf.keras.Input(shape=(None,), name=\"enc_inputs\")\n",
    "\n",
    "    # decoder input (type: keras tensor)\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # encoder padding mask (type: Lambda layer)\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None), name=\"enc_padding_mask\")(enc_inputs)\n",
    "\n",
    "    # decoder padding mask - first sub layer (type: Lambda layer)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(create_look_ahead_mask, output_shape=(1, None, None), name=\"look_ahead_mask\")(dec_inputs)\n",
    "\n",
    "    # decoder padding mask - second sub layer (type: lambda layer)\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(create_padding_mask, output_shape=(1, 1, None), name=\"dec_padding_mask\")(enc_inputs)\n",
    "\n",
    "    # encoder * N (type: keras model)\n",
    "    enc_outputs = encoder(vocab_size, num_layers, dff, d_model, num_heads, dropout,)(inputs=[enc_inputs, enc_padding_mask])\n",
    "\n",
    "    # decoder * N (type: keras model)\n",
    "    dec_outputs = decoder(vocab_size, num_layers, dff, d_model, num_heads, dropout,)(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    # 다음 단어 예측 출력층(단어 개수만큼 출력 존재)\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[enc_inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:26:56.804211Z",
     "start_time": "2022-11-16T05:26:56.790280Z"
    },
    "id": "YRY8cTiYMOSW"
   },
   "outputs": [],
   "source": [
    "# 손실 함수 (cross entropy)\n",
    "\n",
    "# 요약은 문장을 생성해내는 것이고, 이것은 단어 모음에 있는 단어 중\n",
    "# 현재 문장 뒤에 올 단어 하나를 선택하는 다중 클래스 분류 문제이다.\n",
    "# 따라서 cross entropy 함수를 손실함수로 사용한다.\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_OUTPUT_LENGTH -1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3O1CHNkMOSW"
   },
   "source": [
    "## 데이터 정보\n",
    "- size: 2176\n",
    "- sos: 8127\n",
    "- eos: 8128\n",
    "\n",
    "- BATCH_SIZE: 64\n",
    "- MAX_INPUT_LENGTH: 1000\n",
    "- MAX_OUTPUT_LENGTH: 600\n",
    "- VOCAB_SIZE: 8129\n",
    "\n",
    "## 하이퍼파라미터 (논문)\n",
    "- D_MODEL: 256 (512)\n",
    "- NUM_LAYERS: 2 (6)\n",
    "- NUM_HEADS: 8 (8)\n",
    "- DFF = 512 (1024)\n",
    "- DROPOUT: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:27:23.854504Z",
     "start_time": "2022-11-16T05:27:21.548301Z"
    },
    "id": "DYlyTA2SMOSW"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# hyper parameter (논문과 다름)\n",
    "D_MODEL = 256\n",
    "NUM_LAYERS = 2\n",
    "NUM_HEADS = 8\n",
    "DFF = 512\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    num_layers = NUM_LAYERS,\n",
    "    dff = DFF,\n",
    "    d_model = D_MODEL,\n",
    "    num_heads = NUM_HEADS,\n",
    "    dropout = DROPOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:51:34.786633Z",
     "start_time": "2022-11-16T05:51:34.765254Z"
    },
    "id": "9cRyufStMOSW"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(step ** 0.5, step * (self.warmup_steps ** -1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:51:37.343838Z",
     "start_time": "2022-11-16T05:51:37.314278Z"
    },
    "id": "uLOWpXUAMOSW"
   },
   "outputs": [],
   "source": [
    "lr = CustomSchedule(d_model=D_MODEL, warmup_steps=4000) # 학습률\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(BATCH_SIZE, MAX_OUTPUT_LENGTH -1))\n",
    "\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-16T05:30:36.042528Z",
     "start_time": "2022-11-16T05:30:35.390007Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf4VHWTsYFKC",
    "outputId": "264a01d6-04c5-422e-ea8c-8fa6faefc25f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, None, 256)    2081024     ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, None, 256)   0           ['embedding_6[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " Positional_Encoding (Positiona  (None, None, 256)   0           ['tf.math.multiply_6[0][0]']     \n",
      " lEncoding)                                                                                       \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, None, 256)    0           ['Positional_Encoding[0][0]']    \n",
      "                                                                                                  \n",
      " padding_mask (InputLayer)      [(None, 1, 1, None)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " encoder_layer_0 (Functional)   (None, None, 256)    527104      ['dropout_36[0][0]',             \n",
      "                                                                  'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      " encoder_layer_1 (Functional)   (None, None, 256)    527104      ['encoder_layer_0[0][0]',        \n",
      "                                                                  'padding_mask[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,135,232\n",
      "Trainable params: 3,135,232\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder(8129, 2, 512, 256, 8, 0.1).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                save_weights_only=True, verbose=1)\n",
    "\n",
    "\n",
    "EPOCHS=50\n",
    "model.fit(dataset, epochs=EPOCHS, callbacks=[cp_callback], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(user_input):\n",
    "    user_input = re.sub(r\"([?.!,])\", r\" \\1 \", user_input)\n",
    "    user_input = re.sub(\"[^ A-Za-z?.!,$%]+\", '', user_input)\n",
    "    user_input = user_input.strip()\n",
    "    \n",
    "    user_input = START_TOKEN + user_input + END_TOKEN\n",
    "    user_input = tf.keras.preprocessing.sequence.pad_sequences(user_input, maxlen=MAX_INPUT_LENGTH, padding='post')\n",
    "    \n",
    "    output = [START_TOKEN]\n",
    "    \n",
    "    for i in range(MAX_OUTPUT_LENGTH):\n",
    "        predictions = model(inputs=[user_input, output], training=False)\n",
    "        predictions = predictions[:, -1:, :]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "aicoding",
   "language": "python",
   "name": "aicoding"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8bf3ac2c94fcc354b18406fc8c4b51232d8b8a52e22d50326d4fda8485f476a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
